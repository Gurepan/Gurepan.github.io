---
layout:     post
title:      关于大众点评字体加密反爬虫手段的尝试破解
subtitle:   
date:       2020-12-5
author:     Gure
header-img: img/2020-12-5/main.png
catalog: true
tags:
    - 爬虫
---

 本人之前并没有涉足过爬虫相关领域，对于爬虫以及反爬虫手段也一无所知。不过最近身边有人在尝试爬取大众点评网址数据时候遇到了问题，爬下来的数据很多乱码，我很好奇也一起看了看，发现人家对字体做了特殊处理。

![1](https://github.com/Gurepan/Gurepan.github.io/blob/master/img/2020-12-5/1.png?raw=true)



我们可以看到飘红部分1是正常显示，但是我们打开开发者调试时（2）显示的全是一个正方形的框，而标红的3 很明显是导入了一个字体，可以点击进去查看。我们继续查看，发现网页源码如图所示：

![2](https://github.com/Gurepan/Gurepan.github.io/blob/master/img/2020-12-5/2.png?raw=true)

然后我们可以看到所有的正方形框里的东西前缀都是以&#x开头的东西。我们可以猜想，这些东西应该是Unicode码。我们正常的Unicode码应该没有这几个编号，这几个编号对应的字应该是通过自己的字体文件引入的。因此我们直接爬取内容而不对字体做处理时候会出现乱码。

可以看出，解决这个问题关键在于找到引入的特殊字编码与我们正常字符的对应关系。但是在做这个之前，我们必须要先明确一件事，就是是不是所有页面用的特殊字体都是同一套？还是不同页面生成不一样的字体，甚至两次相同请求也会有不一样的字体返回？如果是后者，将会直接增加整个破解工作难度。

万幸的是，经过确认，所有页面用的特殊字体都是相同的几套。也就是说，我们只需要破解一次，就可以将所有页面都反解出来。但是有个问题是，我们不知道这套字体有效期是多久，万一一天过期一次，那么明天都要破解了才能工作，也太累了。但是我发现，字体文件是有缓存的，我特地看了一下缓存有效期：

![3](https://github.com/Gurepan/Gurepan.github.io/blob/master/img/2020-12-5/3.png?raw=true)

显示字体缓存到月末过期，而现在是月初。鉴于人家是大公司需要有良好的用户体验，我有较大的把握确定人家不会中途换字体。因此我们如果破解了会有一个月的时间来爬取内容，应该足够了。

破解的主要工作就是使用OCR对渲染的字体进行识别，这个网上有很多工具这里不多说。因为一个字体也才600多字，自己人工识别一下也是可以的。

**不过在做的时候我发现了一件有趣的事情。**

我把字体文件下载下来进通过`FontCreator`软件进行查看，结果如图所示，所有页面的woff文件的内容都是一样的，仅仅对应的上边Unicode码不一样。

![4](https://github.com/Gurepan/Gurepan.github.io/blob/master/img/2020-12-5/4.png?raw=true)

woff文件相当于是**一个Unicode编码到显示字符的映射表**。显示字符在文件内部是通过`GlyphCoordinates`来规定线段的集合，并最终描述这个字的。也就是说，一个字与另一个字显示一模一样等价于`GlyphCoordinates`相等。为了确认我的想法，我将woff文件解析出来看看。

![5](https://github.com/Gurepan/Gurepan.github.io/blob/master/img/2020-12-5/5.png?raw=true)

结果不出意料，这几个字体文件字体的`GlyphCoordinates`完全一致，仅仅是映射关系做了更改。那么，事实上我们只需要通过OCR来识别一个文件找到`GlyphCoordinates`和字的对应关系并保存起来，就可以给其他字体复用了。甚至我们可以猜想，假使一个月这几个字体过期了，新的字体文件也只会更改Unicode编码到`GlyphCoordinates`的映射，而不会更改里面`GlyphCoordinates`的值，毕竟维护一套字体的成本也是很高的。如果猜想成立的话，我们之后就不需要次次使用OCR识别了，woff文件保存了Unicode编码到显示字符的映射，而我们又解析出了显示字符与真实字符的映射关系。因此就算以后更新了，我们也只需要重新对应一下而已。

解析出来的显示字符与真实字符的映射关系我保存在数据库中，如下所示：

![6](https://github.com/Gurepan/Gurepan.github.io/blob/master/img/2020-12-5/6.png?raw=true)

同时用一个脚本就可以重新生成特殊Unicode编码到对应汉字的关系：

```python
from fontTools.ttLib import TTFont
import mysql.connector


def get_word(filename, dataMap):
    font = TTFont(filename)
    result = font['cmap']
    cmap_dict = result.getBestCmap()
    for key, value in cmap_dict.items():
        k_tmp = str(hex(eval(str(key))))
        b = k_tmp.replace("0x", '')
        glyf = font.get('glyf')
        c = glyf[value]
        cs = str(c.coordinates)
        if cs in dataMap:
            print(b + "->" + str(value) + '->' + dataMap[cs])
        else:
            print("no map" + cs)


def select_data():
    conn = mysql.connector.connect(user='root', password='root', database='dian_ping')
    try:
        cursor = conn.cursor()
        cursor.execute('select position,word from woff ')
        results = cursor.fetchall()
        result_map = {}
        for row in results:
            result_map[row[0]] = row[1]
        return result_map
    except:
        return "error"
    finally:
        conn.close()


data = select_data()
get_word(r"review.woff", data)
```

解决了这个问题同时我们也应该思考，这种情况应该是字体反爬虫手段里面较为简单的了。如果网址想升级手段，仅仅将字体的`GlyphCoordinates`值增加一个干扰值，将某个点移动一点点。这样在人眼的情况下看不出什么，但是对于这种依赖完全匹配来破解的手段来说就已经完全失效了。只能依赖OCR甚至深度学习。不过还好的是，大众点评不在此列，网上我通过收集发现，在两年前就有人发布了`GlyphCoordinates`和字符的对应关系了，对比一下发现和我现在解析的一模一样。看来这次破解以后应该可以用很久了……