---
layout:     post
title:      关于消息系统端到端一致性问题的实践
subtitle:   
date:       2020-11-25
author:     Gure
header-img: img/2020-11-25-EOMP2/post-bg-digital-native.jpg
catalog: true
tags:
    - 一致性
---

前几天写了一篇文章[关于消息中间件端到端一致性问题的思考](https://gurepan.github.io/2020/11/10/EOMP/),有人可能好奇，为啥我那么看重端到端一致性问题，而不是在消息可靠性方面来处理问题。关于这个问题，事实上也是我在生产实践过程一步步踩坑踩来的。

## 问题背景

熟悉我的人可能知道，我前一份工作是在一家物流公司主要负责结算项目，这个项目有几个服务构成，其中的核心流程是一个消息服务：接收计费消息，产出计费结果消息。

![流程概况](https://github.com/Gurepan/Gurepan.github.io/blob/master/img/2020-11-25-EOMP2/%E6%B5%81%E7%A8%8B%E6%A6%82%E5%86%B5.png?raw=true)

其中`运单处理`服务会管理整个运单的生命周期业务，并且在运单发生变动的时候发起一个计费请求消息。比方说一个运单要是经过了`A->B->C`3个状态的转化的话，`运单处理`服务就会向mq发送最新状态A,B,C。结算服务会根据运单的最新状态，加上之前收集的数据，来计算出最新的价格并传导给下游。也就是说，上游服务认为运单状态是`A->B->C`的情况下，本身服务也应该保持状态是`A->B->C`，即保持一致性。将场景抽象出来说，即在异步系统中多条消息场景下，达成一个**共识（consensus）**。这个共识不仅包括订单的三个状态A,B,C不能丢失，同时也包含了其中的顺序是`A->B->C`。缺少或者顺序出错会导致最后实际的订单状态和计费以为的订单状态不一致，最终造成业务出错。而且由于物流事实上是B2B业务，这种出错一般要到计算日才会检查出来，尽管我们设计了一套计费错误运单修复机制来对指定运单同步一致性并重新计费，但是排查哪些运单出了问题以及这些运单哪个环节出了问题难度都相当大。因此这个问题也是我在维护这个服务时候需要重视的问题。

顺序问题也是在异步单调一致性场景下特有的问题。在现在状态分离的设计思想下，一个对象的状态（上下文）通常被抽取并保存在数据库中，而应用则保持为无状态应用并共享数据库中的上下文。这种面向WEB请求的服务，如果是流程的入口，只有并发而没有顺序一说的，它是顺序的创建者，如果一个状态为`A`的运单同时接到将状态改为B,C的两个并发请求。只需要加锁（一般是分布式锁）就无形中给两个请求进行了排序。而如果其在分布式调用中同步调用了其他服务，也就无形间将顺序同步给了其他服务。因此，只有在异步场景下才有这个问题。

同时，排查运单问题难度最大的也是消息顺序问题。顺序问题最简单的解决方案是单队列单消费者，但是这样的话吞吐量明显太差。面对这个问题，可以把使用多个队列并按照运单号`Sharding`（我们用的是`rabbitmq`，用`rocketmq `的话可以使用分区顺序消息）。因此我刚接手时候结构如下：

![流程概况2](https://github.com/Gurepan/Gurepan.github.io/blob/master/img/2020-11-25-EOMP2/%E6%B5%81%E7%A8%8B%E6%A6%82%E5%86%B52.png?raw=true)

PS:其实保证消息顺序一致（其实就是当前消息的上下文一致）还有一种方法，就是上游在发送消息时候发送整个完整上下文，即发送状态C时候发送完整的`A->B->C`链路。但是这样有两个缺点，一个缺点是这样会占用大量带宽，但是这个问题对于我们业务来说影响很小；另外一个缺点是消息的上游可能不止一个，在我们后来对上游项目做了微服务改造之后就会产生多个消息生产者，这些服务之间有可能会用分布式锁来锁定一个运单来进行排序操作，但是他们要想感知整个上下文代价是相当大的。因此这个方案在我们一开始设计系统的时候就被排除了。



## 解决消息投递可靠性

其实上述方案并没有涉及到消息的投递可靠性问题。事实上我们一开始也并没有做关于这方面的努力，仅仅队列本身做了持久化和高可用集群，发送端没有开启发送回调，而且在事务中直接发送消息，不考虑业务和MQ本身一致性问题。接受端开启了自动ACK，相当于就算MQ将消息投递到了消费端，消费端也不一定可以处理它，即无法保证消费端和MQ的一致性。单个消息的一致性都是问题了，更别说多条消息的单调一致性了。这样的设计让我们吃了很大的苦头：因为存在丢消息的隐患，即客户那边少算了一次钱，客户自然不会说什么，但是公司就亏钱了。而且到几个月一次的对账日，光光补帐就让我们哭爹喊娘了。

处理单个消息的一致性，即消息投递可靠性的方案有很多，我选取了本地消息表+消息消费回调+定时扫表的方案来处理：使用本地消息表+事务后发送+定时扫表来解决生产者和MQ的一致性，消费者使用消费记录表来处理幂等问题，用处理后回调来通知生产者这个消息已经达到一致状态，以此来确保一条消息的最终一致性。这样下来，每条消息都可以可靠投递，同时使用单消费者保证消费有序，很完美，对吗？

![流程概况3](https://github.com/Gurepan/Gurepan.github.io/blob/master/img/2020-11-25-EOMP2/%E6%B5%81%E7%A8%8B%E6%A6%82%E5%86%B53.png?raw=true)

事实上，这个方案上线没多久就给了我当头一棒。

这个问题就在于通知是异步的，定时扫表发送也是异步的。

发送端发送消息时候，发送成功了还好说，要是发送失败了，这条消息会等待定时任务来进行异步发送。而在其后面的消息并没有被阻塞住，也就是说其他消息先于这个消息发送了出去，破坏了消息的顺序。这个方案上线前系统表现的情况是会丢消息，这个方案上线以后消息是不丢了，但是顺序乱了。

因此，到了对账日时，错单的数量由原来的百多条减小到了十条以内。但是这几条运单顺序极难溯源，原因很难排查哪一步的问题，因此那次对账，我还是哭爹喊娘，暗无天日。

分布式系统最tricky的问题就是, **问题看起来很普通很简单**; 一些问题总是看起来有简单直接的解法，而一个"简单解"被人查出问题时，也总是看起来可以很简单的就可以把这个挑出的edge case很简单的解决掉; 然而我们会立刻发现解决这个edge case而引入的新步骤会引发新的问题... 如此循环, 直到"简单"叠加到"无法解决的复杂"。

## 几种解决方案

#### 同步发送端发送顺序

上面方案遇到的问题，原因在于发送端吃不准这次发送是否成功就继续发送发送下一条消息。因此，最直接的解决方案，就是发送端在发送一台消息以后阻塞自身，等到消息发送成功以后再继续发送下一条消息。至于如何判断发送成功呢？我们可以用消费端的消费回调来通知。也就是说，我们可以发送一条消息以后就停止发送，直到消费者通知消息消费完成后再继续。嗯，这个方法的确万无一失，可以保证消费的可靠投递以及顺序消费。但是有一个问题，这样做和同步调用有什么区别？消息队列的异步解耦，削峰填谷的功能都被吃了嘛？

![智障](https://github.com/Gurepan/Gurepan.github.io/blob/master/img/2020-11-25-EOMP2/%E6%99%BA%E9%9A%9C.jpg?raw=true)

有的人可能会想，我们可以用MQ的入队列回调来做确认，这样就不需要堵塞至消费端消费完成后，只需要保证入队列了就可以继续下一条，但是这里还有一个问题：MQ的回调通知有可能会丢失，而丢失的结果就是会一直堵塞。

#### 事务消息方案

让我们回到标题，上面问题存在的原因之一是我们想当然的将端到端一致性理解成了处于消息中间件上的两个点之间保存一致性，而将消息系统放在“一个不可靠传输，依赖上层协议来实现可靠传输”的位置上。但是事实上，开启持久化和高可用的队列，本身其实是可靠的。我们之所以认为其不可靠，很大原因是因为它没办法很好的和生产者，消费者之间保持一致性。在上篇文章中我们说到，我们可以设计一套交互规则，让MQ本身可以和生产者，消费者分布形成一致性。我们不妨把消息队列看作是一个临时存放消息的仓库。通过这套交互规则，生产者可以把要发送的消息按照顺序放置在仓库内，消费者可以按照顺序从里面拿取数据。这样一想，消息队列本身是不是就是一个数据库？也就是说，我们发送或者消费一条消息，实际上是操作两个数据库。一个是业务数据库，一个是MQ“数据库”，那么，要这两个数据库达成一致性经常用的方法是什么呢？2PC。也就是说，我们可以对任意一个MQ进行2PC的包装，就可以实现所谓的事务消息。当然为了防止顺序堵塞的情况，我们可以加入MQ定时询问数据状态。这个思路也就是我上一篇文章的流程图，那个流程图我是另外附加了一个存储来存放中间消息，但是后来我发现，可以直接加一个延时队列和原队列配合就可以实现事务消息。

这个方案加上单消费者顺序消费事实上已经可以做到严谨的端到端一致性了，市面上也已经有对于的产品比如`rocketmq `。但是我们最终并没有选用这个思路来做下去。原因一是因为这样对消费者端会存在单点问题。原因二是因为如果消费者端顺序消费的时候忽然出错了（可能是业务原因），除非直接丢入死信队列（这样会破坏顺序），这个队列就会一直阻塞并重试，直到人工干预。原因三是这样的服务实际上是有状态服务，而对这种服务进行扩容要做状态迁移（迁移顺序），比较麻烦。

其实第一个问题解决办法不是没有，最简单的方法就是同时部署两个一样的服务并将单队列数据广播给两个服务进行同时消费，正常情况下会产生两个一样的结果给下游，下游需要做好数据幂等性。这样设计可想而知会造成资源浪费；还有解决方案就是创建2n+1个服务进行广播并同时计算，然后将结果进行广播，通过类似于`raft`选主模式来判断当N+1个节点计算出结果以后由最早算出的节点将最后结果给出，这样可以做到只发一条消息但是还是存在资源浪费。另外的解法可以维护一个后备节点池，由一个高可用的一致性管理工具（如`zookeeper`）来下线失效节点并给替换新节点，但是在实际过程中由于**不存在完美的Failure Detector** ，导致**在保证liveness的情况下(即检测到失败就在另外的机器重启逻辑节点), 无法保证系统中的Singleton角色“在同一时间点”只有一个;** ，也就是存在**Zombie process**，很不巧我们的服务要向下游发送数据，也就是存在副作用。在这种情况下下游服务无法确认消息来自于哪一个进程。好消息是无论消息来自于哪一个进程，他们的结果都是一样的，只需要做好幂等处理即可。不然需要处理**Zombie Fencing**要引入`epoch number`了。

这个问题关键在于第二个和第三个问题，这两个问题对于吞吐量影响比较大，因为这两个问题解决起来麻烦，最后我还是选择了放弃这个方案。

#### 为什么需要单调一致性？

回到这个问题，其实我们已经发现，对于这个问题我们实际上陷入了一个死胡同：消息队列本身保障的是最终一致性，而我们通过一系列手段将其转化为单调一致性，在这个过程我们过多地关注了消息队列这个对象，但是我们事实上可以在乱序的消息处理过程中最终获得有序的上下文。怎么样？有没有想到有个日常经常用的协议就做了这件事？对，就是`TCP`，从乱序中提取出有序数据并给业务处理，还记得`TCP`如何处理乱序的吗？`SYN`+`out_of_order_queue`。因此，我们完全可以放过可怜的MQ，让它老老实实保证好最终一致性即可。我们可以通过对同一个运单的消息进行顺序编号，并且设立暂存区。假设我们现在对Y运单的消息处理编号是n,来一个Y运单的计费消息编号是N+1，那么恭喜它顺序没有问题，可以正常处理。如果来的消息是N+2，那么不好意思，麻烦您再等等，等序号是N+1的消息进来我们再一起处理。这个过程不会阻塞其他消息的处理，也不要求消息有序传输，因此可以方便扩容。麻麻再也不用担心我的吞吐量啦！唯一一点需要担心是在并行处理消息的时候可能有两个进程处理同一个消息的不同序列消息情况，这种情况可以用分布式锁来处理。像我们这种大部分消息都是顺序的，间隔也比较开的情况（几个月冲突几百条），很简单的使用数据库的`for update`给运单号加锁即可。

听上去很完美，是……吗？？

这里面有一个小麻烦：我们对于单个运单号每次+1的序号怎么来？尤其是对于上游是微服务架构，那么需要一个性能好，可用性高，一致性强，每次要自增1的ID生成器。更可怕的是，这个ID生成器要支持事务！！不然要是领取了一个号，执行业务时候失败，顺序就再也连接不上了，它的后续消息处理，将永远停留在暂存区。

或许我们可以回头看看业务，再将放开一些？

作为最终一致性上的最后一环，我们的下游，账单系统，是否可以在有限时间内容忍一个有可能是错的中间结果？当然，在过了这个时间窗口以后结果是对的。对于这个问题，我们业务上也给出意见：由于我们的的出账日是T+X，可以容忍X天内的数据出错。

也就是说，我们终于可以摆脱掉最后一道枷锁，不必执着于产出**一个当前条件下正确的结果**，也就是说，对于计费而言，**没必要构建一个一定的正确有序上下文，只需要保证最终的上下文是正确有序的，并在此基础上计算出正确的值即可**。

## 最终解决方案

在上一个方案中，我们放弃了对消息投递的单调一致性的要求，使用单消息的可靠投递加上应用上层的协议就实现了对每次请求时端到端一致性的需求。这个方案也使我们的应用摆脱了有状态的弊病，可以放心大胆地进行扩容。但是我们也看到了其中的弊病：这个顺序状态并没有消除，只是转移到上游的序列生成逻辑中去了，而且这段逻辑要求线性一致，而不是最终一致，这给整个系统带来更大的难度。

让我们回过头想想，为什么我们的系统要求上游传递的消息要求如此严格的顺序ID？为了在每次计费请求时候保持和上游的状态是单调一致的，为了保持单调一致我们需要使用阻塞来同步状态。为什么要保证和上游的状态是单调一致的？因为要计算出此时“正确”结果给下游系统。那么，我们上面情况以及说了，我们的下游是否可以在有限时间内容忍一个有可能是错的中间结果？当然可以。如此看来，我们**“为了保持单调一致我们需要使用阻塞来同步状态”**这个前提已经不存在了，也就是说，我们没必要使用一个强一致，自增1的顺序ID来同步状态。因为不存在阻塞，我们保证运单的顺序标志按照趋势递增即可，最简单的办法是业务时间，或者业务ID。

于是我们拥有了新的解决方案：假设一个运单状态转移经过了`A->B->C`3个状态，上游会发送A,B,C三个消息，同时带上业务时间。我们只需要保证它们最终都可以被消费到，消费顺序我们可以容忍不一致。每次消息都需要触发计费逻辑，同时将变动结果传递给下游。比如C消息先消费，那么只计算运单在C状态的结果，此时来了A消息，则与上次的C消息一起可以构建”A->C“的环境，对比原结果计算出最新结果。此时来了B消息，则与上次的A,C消息一起可以构建”A->B->C“的环境，对比原结果计算出最新结果。这样一来，计费消息与上游系统最终达成了多消息下的端到端一致性。下游的账单系统也最终获得了正确的结果。通过这个设计，我们放弃了所有的阻塞同步，获得了性能无限扩展的能力。

这里面唯一的问题就在于，这样设计对于计费逻辑是一个大考验：我们需要对比原上下文和新上下文才能取得最后结果的改动，而可以预见到随着状态的增长，这种对比的复杂度是爆炸性的。对于编码及其容易出错。但是这一部分可以通过业务特点以及代码设计来减小复杂度，根据我们业务的设计，复杂度可以约束在O（1）级别，而且日常开发中可以不用关心这一点，只需要关注业务开发即可。

这个方案是我们在当前条件下最终的解决方案，但是如你所见，这并不能算是一个普世的方案，优点缺点一样明显。我们使用这个也不过恰好场景合适而已，**技术没有银弹**，遇到具体场景仍然要具体分析，合适的才是最好的。

PS：尽管这个方案没必要对消息数据进行分区也可保证最终结果的准确性。但是一般情况下我们会将消息数据和对应的消费者进行分区，同个分区内的消费者互为backup，这样即可以避免单点问题，稍微分下区也可以减少并发时候的锁冲突和锁等待。